---
title: "House Prices in New York"
author: "Group CC901E2: SID's"
institute: "The University of Sydney"
format:
  revealjs:
    self-contained: true
    fig-format: retina
    toc: true
    toc-depth: 1
    number-sections: false
    code-line-numbers: false
    slide-number: c
    scrollable: true
    pdf-max-pages-per-slide: 1
    history: false # don't add each slide to browser history
    
execute:
  echo: false
  warning: false # do not show warning in the chunk
  message: false # do not show message in the chunk
---


# Introduction


```{r message=FALSE}
# Load packages here
library(tidyverse)
library(ggplot2)
library(ggfortify)
library(dplyr)
library(ISLR)
library(sjPlot)
library(leaps)
library(GGally)
library(caret)
```

```{r}
# The original data is loaded here
#| output: FALSE
house = read.delim("housing-prices-ge19.txt", 
                    header = TRUE)
# convert data types
house = house %>% mutate (
  Waterfront = as.factor(Waterfront),
  New.Construct = as.factor(New.Construct),
  Central.Air = as.factor(Central.Air),
  Fuel.Type = as.factor(Fuel.Type),
  Heat.Type = as.factor(Heat.Type),
  Sewer.Type = as.factor(Sewer.Type),
  Test = as.factor(Test)
)

# Remove the Test column, because we do not have the metadata
house = house %>%
  select(-Test)

# clean names
# house = janitor::clean_names(house)
```


## Topics and Methodology


```{r}
# I would suggest this part using bullet points in this section

# if one page cannot fit, can use more than one page by "##"

# Indeed, if you put too many words, they even could not fit in one page
```


- The aim for the model is to predict the House Price in New York City
- The data set, House Prices in New York was  obtained from Group Assignment Git Hub Page
-  However, the data set was original sourced from the mosaic Data package in R, Data on houses in Saratoga County, New York, USA in 2006 (SaratogaHouses) collected by Candice Corvetti
  
- The data set contains information about New York Houses including prices of Houses (in Us Dollars), lot size of the house (acres),  number of bedrooms and bathrooms and the type of ultilites in the house, for example the type of heating system

- Price is the dependent variable the model is predicting, with the all other variables except the test variable being used to predict Price
- the test variable in the data set  was excluded from the model because we do not know the meaning of the  variable




# Basic Summary of the Data

## Mean, SD, Max, Min, etc.

This data set does not contain any missing data however, outliers do exist in this data set. For example a 0- acre lot size cannot exist. Also, it is unlikely that $5000 USD would be enough to purchase a House in New York in 2006.


```{r}
groups = c("price", "lot_size", "age", "land_value", "pct_college", "living_area", "bedrooms", "rooms")
summary = house %>%
  select(Price, Lot.Size, Age, Land.Value, Pct.College, Living.Area, Bedrooms, Rooms) %>%
  summarise_all(list(mean,
                     sd,
                     max,
                     min))
summaries = c(rep(c("Mean", "SD","Max", "Min"), each=8))
columns= c(rep(groups, 4))
summary_data = as.numeric(summary[1,])
summary_frame <- data.frame(summaries, columns, summary_data)

summary_wide = summary_frame %>%
  tidyr::pivot_wider(
    id_cols = columns,
    names_from = summaries,
    values_from = summary_data
  )
summary_wide$Mean = round(summary_wide$Mean, 2)  
summary_wide$SD = round(summary_wide$SD, 2)
summary_wide$Max = format(round(summary_wide$Max, 2), nsmall = 2)
summary_wide$Min = format(round(summary_wide$Min, 2), nsmall = 2)
knitr::kable(summary_wide)
```

## Scatterplot
Based on the scatter plot, Living Area is the most likely to have linear relation with House Price. Thus,it will be the most important variable in the model to predict house prices.  

```{r}
house %>%
  drop_na() %>%
  pivot_longer(cols = c(Lot.Size, Age, Land.Value, Pct.College, Living.Area, Bedrooms, Rooms),
               names_to = "variables", values_to = "values") %>%
  ggplot()+
  aes(x= values, y = (Price)/1000)+
  geom_point(size = 0.3, alpha=0.3)+
  facet_wrap(~ variables, nrow=2, scales = "free_x")+
  theme_bw()+
  labs(x= "Variables", y = "House Price (Thousand USD)")

```

## Boxplot 

The number of rooms is an important factor during the process of purchasing a house. A box plot was  used to visualize the distribution of housing prices based on the number of rooms. The box plot demonstrates an increase in the number of rooms of a house,  increases the average house price.  


```{r}
house %>%
  mutate(price = house$Price/1000)%>%
  ggplot() +
  aes(x = as.factor(Rooms), y= price, color = Rooms) +
  geom_boxplot() +
  theme_classic() +
  labs(x = "Number of Rooms", y = "House Price (Thousand USD)")
```

# Simple Regression

## How did you select the model


```{r}
# difference begins
house_num = subset(house,select = c(Price, Lot.Size, Age, Land.Value,Living.Area, Pct.College, Bedrooms, Fireplaces, Bathrooms, Rooms))
house_num
```

```{r}
# GGally::ggpairs(house_num)
# a little bit messy
```

```{r}
cor_house = cor(house_num)

melted_cor_house = cor_house %>%
 data.frame() %>%
 rownames_to_column(var = "var1") %>%
 gather(key = "var2", value = "cor", -var1)
```

```{r}
ggplot(data = melted_cor_house, 
 aes(x=var1, y=var2, fill=cor)) +
 geom_tile() + theme_minimal(base_size = 22)+
 scale_fill_gradient2(
 low = "blue", high = "red", mid = "white",
 midpoint = 0, limit = c(-1,1)) +
 theme(axis.text.x = element_text(angle = 90))
```

```{r}
qtlcharts::iplotCorr(house_num)
```


Since our depend variable is 'Price', based on these two graph, it can easily found that the variable 'Living.Area' may be the most affected by 'Price' (0.71, with dark pink).

*Dependent variable*: Price

*Independent variable*: Living.Area


## Assumption checking

The residuals $\varepsilon_i$are iid${\mathcal N}(0,\sigma^2)$ and there is a linear relationship between y and x.

```{r}
lm1 = lm(Price ~ Living.Area, 
 data = house)
lm1
```

```{r}
library(ggfortify)
autoplot(lm1, which = 1:2) + theme_bw()
```


-   Linearity: The Auxiliary line is reasonably well plotted like a straight line (with no obvious curve), so there is no obvious pattern in the residual vs fitted values plot.

-   Homoskedasticity: It appears the residuals are getting spread-out and do not appear to be fanning out or changing their variability over the range of the fitted values so the constant error variance assumption is met.

-   Normality: in the QQ plot, the points are reasonably close to the diagonal line. Although there seems to be some outliers exist, the data-set is relatively large(with 1733 observations). Thus, the normality assumption is approximately satisfied.


## Simple regression model


```{r}
plot(Price ~ Living.Area, 
 data = house)
abline(lm1, lwd = 3, col = "red")
```

*Conclusionï¼š*
As the assumption all met, it can be concluded that our simple estimated model is $\widehat{Price} = `r format(lm1$coefficients[[1]], scientific = FALSE, nsmall = 3)` + `r round(lm1$coefficients[[2]],3)` \times Living.Area$

# Multiple Regression

## Backward AIC

```{r}
# Sample Mean of Price
M0_back = lm(house$Price ~ 1, data = house)
# Price against everything except itself
M1_back = lm(house$Price ~ ., data = house)

# Rounds summary of M0 to 3 signficant figures
round(summary(M0_back)$coef,3)
# Rounds summary of M1 to 3 signficant figures
round(summary(M1_back)$coef,3)

# Dropping of variables backwards until lowest AIC is reached
step.back.aic = step(M1_back,
                     direction = "backward",
                     trace = FALSE)
summary(step.back.aic)
```




## Assumption Checking for Backward AIC

```{r}
house %>%
  drop_na() %>%
  pivot_longer(cols = c(Lot.Size, Age, Land.Value, Living.Area, Bedrooms, 
                        Bathrooms, Rooms),
               names_to = "variables", values_to = "values") %>%
  ggplot()+
  aes(x= values, y = (Price)/1000)+
  geom_point(size = 0.3, alpha=0.3)+
  facet_wrap(~ variables, nrow=2, scales = "free_x")+
  theme_bw()+
  labs(x= "Variables", y = "House Price (Thousand USD)")
autoplot(step.back.aic, which = c(1,2))
```


## Forward AIC

```{r}
# Sample Mean of Price
M0_fwd = lm(house$Price ~ 1, data = house)
# Price against everything except itself
M1_fwd = lm(house$Price ~ ., data = house)

# Rounds summary of M0 to 3 significant figures
round(summary(M0_fwd)$coef,3)
# Rounds summary of M1 to 3 significant figures
round(summary(M1_fwd)$coef,3)

# Dropping of variables forwards until lowest AIC is reached
step.fwd.aic = step(M0_fwd, scope = list(lower = M0_fwd, upper = M1_fwd),
                     direction = "forward",
                     trace = FALSE)
summary(step.fwd.aic)
```


## Assumption Checking for Forward AIC

```{r}
house %>%
  drop_na() %>%
  pivot_longer(cols = c(Living.Area, Land.Value, Bathrooms,
                        Lot.Size, Age, Rooms, Bathrooms),
               names_to = "variables", values_to = "values") %>%
  ggplot()+
  aes(x= values, y = (Price)/1000)+
  geom_point(size = 0.3, alpha=0.3)+
  facet_wrap(~ variables, nrow=2, scales = "free_x")+
  theme_bw()+
  labs(x= "Variables", y = "House Price (Thousand USD)")
autoplot(step.fwd.aic, which = c(1,2)) 
```


## Exhaustive search

```{r}
# Sample Mean of Price
M0_exh = lm(house$Price ~ 1, data = house)
# Price against everything except itself
M1_exh = lm(house$Price ~ ., data = house)

# Rounds summary of M0 to 3 significant figures
round(summary(M0_exh)$coef,3)
# Rounds summary of M1 to 3 significant figures
round(summary(M1_exh)$coef,3)

# Exhaustive search
set.seed(5)
library(leaps)
exh = regsubsets(Price ~ ., data = house, nvmax = 11)
x = summary(exh)

x$rsq

summary(exh)$outmat
```


## Assumption Checking for Exhaustive Search

```{r}
# GGally::ggpairs(house) + theme_bw(base_size = 22)
# autoplot(exh, which = c(1,2)) 
```



# Discussion: Model evaluations

In-sample and out-of-sample performance


## Our models - Simple regression

$$\widehat{Price} = `r format(lm1$coefficients[[1]], scientific = FALSE, nsmall = 3)` + `r round(lm1$coefficients[[2]],3)` \times Living.Area$$


```{r}
plot(Price ~ Living.Area, 
 data = house)
abline(lm1, lwd = 3, col = "red")
```


## Our models - Stepwise regression

::: columns
::: {.column width="50%"}

- Backward stepwise regression models: 


```{r}
knitr::kable(round(step.back.aic$coefficients,3))
```

:::

::: {.column width="50%"}

- Forward stepwise regression models: 


```{r}
knitr::kable(round(step.fwd.aic$coefficients,3))
```

:::
:::


## In-sample performance
- $r^2$: coefficient of determination
- It shows what percentage of the total variation of observed data is explained by our linear regression models - how well our linear models fit the original data
- Range $[0, 1]$

##
- Compare $r^2$ of the three models:


```{r}
# r^2 of simple model
rsquare_simple = summary(lm1)$r.square

# r^2 of backward
rsquare_back = summary(step.back.aic)$r.square

# r^2 of forward
rsquare_fwd = summary(step.fwd.aic)$r.square

# make a table
models = c("simple regression", "Backward stepwise", "Forward stepwise")
r_square = c(rsquare_simple, rsquare_back, rsquare_fwd)

rsquare_table <- data.frame(models, r_square)

## round into three decimal
rsquare_table = rsquare_table %>%
  mutate_if(is.numeric, 
            round, 
            digits = 3)

knitr::kable(rsquare_table, col.names = c("Models", "r square"))


```


- The $r^2$ of backward and forward stepwise regression models seem to be the same; -
- Backward and forward stepwise regression models fit the data better than the simple regression model.
- Overfiting?



## Out-of-sample performance
* $k$-fold cross validation estimation
    + Separate the data set randomly into different parts with equal size;
    + Leaving one part out, and use other parts to build a model;
    + Use the built model to predict observations left out;
    + Evaluate error rates with the actual left out data to see how well the model performs;
      - Error rates: Mean Absolute Error (MAE) and Root Mean Square Error (RMSE);
      - The smaller the error rates the better.
    + Repeat $k$ times, and average the error rate over all $k$ runs.


## 10-Fold cross validation

```{r}
# set 10 folds cross validation
tr_ctrl = trainControl(
    method = "cv", number = 10,
    verboseIter = FALSE
)

# simple
cv_simple = train(
        Price ~ Living.Area, house,
        method = "lm",
        trControl = tr_ctrl
        )

# extract the results
result_simple = cv_simple$results


# backward
cv_back = train(
        Price ~ Lot.Size + Waterfront + Age + Land.Value + 
                New.Construct + Central.Air + Heat.Type + Living.Area + Bedrooms +                 Bathrooms + Rooms,
        house,
        method = "lm",
        trControl = tr_ctrl
        )

result_back = cv_back$results


# forward
cv_fwd = train(
        Price ~ Living.Area + Land.Value + Bathrooms + 
                Waterfront + New.Construct + Heat.Type + Lot.Size + Central.Air + 
                Age + Rooms + Bedrooms,
        house,
        method = "lm",
        trControl = tr_ctrl
        )

result_fwd = cv_fwd$results

# union data frame
# result_union = union(result_simple, result_back)
# result_union = union(result_union, result_fwd)
# result_union

```

```{r}
# select the column needed to make new dataframe

models = c("simple", "Backward", "Forward")
frame_models = c( rep(models, each = 3) )

errors = c("RMSE", "R square", "MAE")
frame_errors = c( rep(errors, length(models)) )

frame_data = c(result_simple[1,2], result_simple[1,3], result_simple[1,4],
               result_back[1,2], result_back[1,3], result_back[1,4],
               result_fwd[1,2], result_fwd[1,3], result_fwd[1,4]
                  )

evaluation_selected <- data.frame(frame_models, frame_errors, frame_data)


# visualise the errors
evaluation_selected %>%
  ggplot() +
  aes(x = frame_models, y = frame_data) +
  facet_wrap(~frame_errors, scales = "free_y") +
  geom_boxplot() + 
  labs(x = "", y = "")


```


## Refitting Data after Cross Validation

```{r}
lm.fin = lm(Price ~ Living.Area + Land.Value + Bathrooms + 
                Waterfront + New.Construct + Heat.Type + Lot.Size + Central.Air + 
                Age + Rooms + Bedrooms, data = house)
summary(lm.fin)
```





## Results
As we can see from the plots:

* $r^2$: backward and forward stepwise regression models have HIGHER $r^2$ than the simple model;
    + The $10$-fold cross validation results is CONSISTENT with the in-sample performance $r^2$ results
    
* MAE and RMSE: back ward and forward stepwise regression models have LOWER MAE and RMSE than the simple model.

* Both in-sample and out-of-sample performance evaluations show that back ward and forward stepwise regression models are _better_ than simple regression models. 

* In addition, backward and forward stepwise models behave AS WELL AS each other.


## Limitation
- When repeating evaluation of performance many times as in the $10$-fold cross validation, $r^2$ of simple regression model is relatively low, which indicates that it may not be a good model to predict house prices of this dataset;
- The $r^2$ values of backward and forward stepwise models are slightly better, with around $65%$ variation of the original data explained by the models, which is acceptable;
- However, they all have relatively high MAE and RMSE (why?)




# More Discussion

Model stability

## Model stability

Your code and content

# References

## 

-   Data collected by Candice Corvetti and used in the "Stat 101" case study "How much is a Fireplace Worth". See also https://www.saratogacountyny.gov/departments/real-property-tax-service-agency/
-   El Hattab, Hakim, and JJ Allaire. 2017. Revealjs: R Markdown Format for Reveal.js Presentations. https://github.com/rstudio/revealjs.

